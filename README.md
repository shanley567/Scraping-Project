# Web Scraper

A modular, productionâ€‘style Python ETL pipeline that uses Selenium to extract heading structures (`h1`â€“`h6`) from any webpage, transform them into a hierarchical tree, and save the results as structured data.

This project now includes:

* **Dynamic userâ€‘agent rotation**
* **Modular architecture** (browser, extract, transform, load)
* **Humanâ€‘like browser behaviour**
* **Configurable ETL flow**
* **JSON output for downstream analysis**

Itâ€™s designed as a foundation for a larger scraping + analytics system.

## Project Overview

This scraper launches a real Chrome browser, loads a target webpage, extracts all heading tags in the order they appear, and builds a **true hierarchical representation** of the page structure.

The project follows a clean ETL pattern:

* **Extract:** Selenium retrieves raw headings
* **Transform:** Headings are converted into a nested hierarchy
* **Load:** Output is saved as JSON for analysis or downstream pipelines

This structure mirrors real production scraping systems and is designed for future expansion.

## Features

âœ” Modular Architecture

The scraper is split into dedicated modules:

* `browser.py` â†’ Selenium setup + userâ€‘agent rotation
* `extract.py` â†’ DOM extraction logic
* `transform.py` â†’ hierarchy builder
* `load.py` â†’ JSON output
* `main.py` â†’ ETL orchestration

This makes the project maintainable, testable, and extensible.

âœ” Dynamic Userâ€‘Agent Generation

Each run uses a realistic, randomly generated Chrome userâ€‘agent string.

âœ” Humanâ€‘Like Interaction

Scrolling, waits, and timing randomness reduce false positives and improve stability.

âœ” Hierarchical Output

Headings are transformed into a nested structure that reflects the actual page layout

## Installation & Setup

1. Clone the repository

```powershell
git clone https://github.com/yourusername/bbc-scraper.git
cd bbc-scraper
```

2. Create a virtual environment

```powershell
python -m venv venv
```

Activate the environment (PowerShell)

```powershell
.\venv\Scripts\Activate.ps1
```

To deactivate:

```powershell
deactivate
```

4. Install dependencies

```powershell
pip install -r requirements.txt
```

## â–¶ï¸ Running the Scraper

Run the ETL pipeline:

```powershell
python -m scraper.main
```

Or specify a custom URL/output path by editing `config.yaml` (coming in Step 2).

Output will be saved to:

data/headings.json

Example console output:

H1: BBC
  H2: Top Stories
    H3: World
    H3: Business
  H2: Sport
    H3: Football
    H3: Cricket

ğŸ“ Project Structure

bbc_scraper/
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ browser.py        # Selenium driver + user-agent rotation
â”‚   â”œâ”€â”€ extract.py        # Extract raw headings
â”‚   â”œâ”€â”€ transform.py      # Build hierarchical structure
â”‚   â”œâ”€â”€ load.py           # Save output to JSON
â”‚   â””â”€â”€ main.py           # Orchestrates the ETL pipeline
â”‚
â”œâ”€â”€ config.yaml           # Configurable URL + output path (Step 2)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

This structure mirrors real ETL and scraping frameworks.

### ğŸ”® Roadmap

Planned enhancements:

* Add `config.yaml` integration
* Add structured logging
* Add retry logic + error handling
* Add CLI arguments (`--url`, `--output`)
* Add unit tests (pytest)
* Add Docker container
* Add scheduling (cron / Task Scheduler / Airflow)
* Add NLP analysis (topic modelling, sentiment)
* Add a Streamlit dashboard for visualisation

### ğŸ§‘â€ğŸ’» Skills Demonstrated

* Selenium browser automation
* Dynamic userâ€‘agent rotation
* ETL pipeline design
* Modular Python architecture
* JSON data modelling
* Git/GitHub workflow
* Portfolioâ€‘ready documentation

### Dev Notes

Virtual Environment

```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
deactivate
```
